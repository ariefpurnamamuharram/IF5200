{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "functional-conditions",
   "metadata": {},
   "source": [
    "# IF5200 - Modeling Notebook\n",
    "___\n",
    "Group: 8<br>\n",
    "Project: Automated Chest X-Ray Report Generator in Bahasa Indonesia with the Use of Deep Learning<br>\n",
    "Team members: Arief Purnama Muharram, Hollyana Puteri Haryono, Abassi Haji Juma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-isolation",
   "metadata": {},
   "source": [
    "## A. Print library version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-signal",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('tqdm version:', __import__('tqdm').__version__)\n",
    "print('matplotlib version:', __import__('matplotlib').__version__)\n",
    "print('seaborn version:', __import__('seaborn').__version__)\n",
    "print('pandas version:', __import__('pandas').__version__)\n",
    "print('scikit-learn version:', __import__('sklearn').__version__)\n",
    "print('pillow version:', __import__('PIL').__version__)\n",
    "print('torch version:', __import__('torch').__version__)\n",
    "print('torchvision version:', __import__('torchvision').__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-costs",
   "metadata": {},
   "source": [
    "## B. Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-lawrence",
   "metadata": {},
   "source": [
    "### 1. TrainUtils class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-cornwall",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class TrainUtils:\n",
    "    \n",
    "    def __init__(self, model, \n",
    "                 loss_fn: str, \n",
    "                 optimizer: str, \n",
    "                 learning_rate: float = 1e-3, \n",
    "                 device: str = None):\n",
    "        \n",
    "        super(TrainUtils, self).__init__()\n",
    "        \n",
    "        # Set model\n",
    "        self.model = model\n",
    "        \n",
    "        # Set loss function\n",
    "        if loss_fn not in ['CrossEntropyLoss']:\n",
    "            raise ValueError('Loss function is not supported!')\n",
    "        else:\n",
    "            if loss_fn == 'CrossEntropyLoss':\n",
    "                self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Set optimizer\n",
    "        if optimizer not in ['Adam', 'SGD']:\n",
    "            raise ValueError('Optimizer is not supported!')\n",
    "        else:\n",
    "            if optimizer == 'Adam':\n",
    "                self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            elif optimizer == 'SGD':\n",
    "                self.optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Set device\n",
    "        if device is not None:\n",
    "            self.device = torch.device(device)\n",
    "            print('Using GPU!\\n')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "            print('Using CPU!\\n')\n",
    "\n",
    "    def train(self, dataloader, \n",
    "              print_log: bool = False):\n",
    "        \n",
    "        model = self.model\n",
    "        loss_fn = self.loss_fn\n",
    "        optimizer = self.optimizer\n",
    "        device = self.device\n",
    "        \n",
    "        loss_history = []\n",
    "        \n",
    "        for batch, (X, y) in enumerate(tqdm(dataloader)):\n",
    "            # Switch to train mode\n",
    "            model.train()\n",
    "            \n",
    "            # Send tensors to the device\n",
    "            X, y, model = X.to(device), y.to(device), model.to(device)\n",
    "            \n",
    "            # Compute loss (error)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            \n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Append batch loss history\n",
    "            if batch % 100 == 0:\n",
    "                loss_history.append([batch, loss])\n",
    "                \n",
    "        # Print loss history\n",
    "        if print_log == True:\n",
    "            print('Loss over batches:')\n",
    "            print(' Batch\\tLoss')\n",
    "            for item in loss_history:\n",
    "                print(f' {item[0]}\\t{item[1]:>7f}')\n",
    "    \n",
    "        # Return loss history\n",
    "        return (loss_history)\n",
    "\n",
    "    def test(self, dataloader, \n",
    "             print_log: bool = False):\n",
    "        \n",
    "        model = self.model\n",
    "        loss_fn = self.loss_fn\n",
    "        device = self.device\n",
    "        \n",
    "        size = len(dataloader.dataset)\n",
    "        num_batches = len(dataloader)\n",
    "        \n",
    "        # Switch to eval mode\n",
    "        model.eval()\n",
    "        \n",
    "        test_loss, correct = 0, 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X, y in tqdm(dataloader):\n",
    "                # Send tensors to the device\n",
    "                X, y, model = X.to(device), y.to(device), model.to(device)\n",
    "                \n",
    "                # Make prediction\n",
    "                pred = model(X)\n",
    "            \n",
    "                test_loss += loss_fn(pred, y).item()\n",
    "                correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    \n",
    "        test_loss /= num_batches\n",
    "        correct /= size\n",
    "        \n",
    "        # Print test accuracy and test lost\n",
    "        if print_log == True:\n",
    "            print(f'Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}')\n",
    "        \n",
    "        # Return test accuracy\n",
    "        return (correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-military",
   "metadata": {},
   "source": [
    "### 2. build_model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-accountability",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "def build_model(pretrained, \n",
    "                d_class: int = 2):\n",
    "    \n",
    "    # Load model\n",
    "    model = pretrained\n",
    "    \n",
    "    if d_class <= 1:\n",
    "        raise ValueError('Can not less than 2 classes!')\n",
    "    \n",
    "    # Setup final classification layer\n",
    "    model.fc = nn.LazyLinear(d_class)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-friendship",
   "metadata": {},
   "source": [
    "### 3. PreprocessDataLoader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-mambo",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class PreprocessDataLoader(Dataset):\n",
    "    \n",
    "    def __init__(self, data, image_path):\n",
    "        \n",
    "        super(PreprocessDataLoader, self).__init__()\n",
    "        \n",
    "        self.data = data\n",
    "        self.image_path = image_path\n",
    "        self.transform = T.Compose([\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        data = self.data.iloc[:, 0:len(self.data)].iloc[idx]\n",
    "        \n",
    "        # Image preprocessing\n",
    "        image = data['image']\n",
    "        image = Image.open(f'{self.image_path}/{image}').convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        label = data['label']\n",
    "        \n",
    "        return (image, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-adapter",
   "metadata": {},
   "source": [
    "### 4. train_wrapper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-ethics",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = 'models/'\n",
    "LOG_DIR = 'logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-november",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time as timer\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def train_wrapper(model, trainer, \n",
    "                  train_dataloader, val_dataloader, test_dataloader, \n",
    "                  epochs=10, saved_model_name='model.pth', log_name='log.txt'):\n",
    "    \n",
    "    model_path = os.path.join(MODEL_DIR)\n",
    "    log_path = os.path.join(LOG_DIR)\n",
    "    fig_path = os.path.join(LOG_DIR)\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "        \n",
    "    if not os.path.exists(log_path):\n",
    "        os.makedirs(log_path)\n",
    "        \n",
    "    model_path = os.path.join(model_path, saved_model_name)\n",
    "    log_path = os.path.join(log_path, log_name)\n",
    "    \n",
    "    train_history = []\n",
    "    \n",
    "    with open(log_path, 'w') as fh:\n",
    "        \n",
    "        # Write log header\n",
    "        fh.write('epoch\\ttrain_acc\\ttest_acc\\n')\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # Print epoch status\n",
    "            print(f\"Epoch {epoch+1} out of {epochs}\\n ------------\")\n",
    "            \n",
    "            start = timer.time()\n",
    "            \n",
    "            # Train model\n",
    "            trainer.train(train_dataloader, print_log=False)  \n",
    "            \n",
    "            # Get elapsed time\n",
    "            elapsed_time = timer.time() - start\n",
    "            print(f\"Training time: {elapsed_time:>.2f} seconds\")\n",
    "            \n",
    "            start = timer.time()\n",
    "            \n",
    "            # Evaluate model: get training accuracy \n",
    "            train_accuracy = trainer.test(val_dataloader, print_log=False)\n",
    "            \n",
    "            # Get elapsed time\n",
    "            elapsed_time = timer.time() - start\n",
    "            print(f\"Validation time: {elapsed_time:>.2f} seconds\")\n",
    "            \n",
    "            start = timer.time()\n",
    "            \n",
    "            # Evaluate model: get testing accuracy\n",
    "            test_accuracy = trainer.test(test_dataloader, print_log=False)\n",
    "            \n",
    "            # Get elapsed time\n",
    "            elapsed_time = timer.time() - start\n",
    "            print(f\"Testing time: {elapsed_time:>.2f} seconds\")\n",
    "            \n",
    "            # Append epoch train history\n",
    "            train_history.append([epoch, train_accuracy, test_accuracy])\n",
    "            \n",
    "            # Write training log\n",
    "            fh.write(f'{epoch}\\t{train_accuracy}\\t{test_accuracy}\\n')\n",
    "            \n",
    "            # Save model\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f\"Model {model_path} stored!\\n\")\n",
    "            \n",
    "    train_history = pd.DataFrame(train_history, columns=['epoch', 'train_acc', 'test_acc'])\n",
    "    train_history['epoch'] = train_history['epoch'].apply(lambda x: str(x))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.figure()\n",
    "    sns.lineplot(data=train_history, x='epoch', y='train_acc', label='Train Accuracy', color='#5f0f40')\n",
    "    sns.lineplot(data=train_history, x='epoch', y='test_acc', label='Test Accuracy', color='#fb8b24')\n",
    "    plt.title(f'{saved_model_name} Accuracy History over Epochs\\n', fontdict={\n",
    "        'fontsize': 15, 'fontweight': 'bold'\n",
    "    })\n",
    "    plt.xlabel('Epoch', fontdict={\n",
    "        'fontsize': 10\n",
    "    })\n",
    "    plt.ylabel('Accuracy', fontdict={\n",
    "        'fontsize': 10\n",
    "    })\n",
    "    plt.savefig(os.path.join(fig_path, f'{log_name.split(\".\")[0]}.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-protest",
   "metadata": {},
   "source": [
    "## C. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-klein",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment global variables\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drawn-philippines",
   "metadata": {},
   "source": [
    "### 1. Load labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-context",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df_labels = pd.read_csv('datasets/labels_cxr-images.csv', sep=',')\n",
    "df_labels.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-mandate",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "still-hierarchy",
   "metadata": {},
   "source": [
    "### 2. Experiment #1: Cardiomegaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-construction",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_dataset, test_dataset = train_test_split(\n",
    "    df_labels, test_size=0.3, random_state=0\n",
    ")\n",
    "\n",
    "train_dataset, val_dataset = train_test_split(\n",
    "    train_dataset, test_size=0.3, random_state=0\n",
    ")\n",
    "\n",
    "def prepare_dataset(dataset, image_path: str = 'images'):\n",
    "    dataset = dataset[['Filename_Segment2', 'Cardiomegaly']]\n",
    "    dataset = dataset.rename(columns={'Filename_Segment2': 'image', 'Cardiomegaly': 'label'}, errors='ignore')\n",
    "    dataset = PreprocessDataLoader(dataset, image_path)\n",
    "    return dataset\n",
    "\n",
    "train_dataset = prepare_dataset(train_dataset, 'datasets/data_cxr-images_128x128')\n",
    "val_dataset = prepare_dataset(val_dataset, 'datasets/data_cxr-images_128x128')\n",
    "test_dataset = prepare_dataset(test_dataset, 'datasets/data_cxr-images_128x128')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-capability",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mechanical-blink",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import device\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "model = build_model(resnet50(weights=ResNet50_Weights.DEFAULT))\n",
    "trainer = TrainUtils(model, 'CrossEntropyLoss', 'Adam', learning_rate=LEARNING_RATE, device=device('cuda:3'))\n",
    "\n",
    "train_wrapper(model, trainer,\n",
    "             train_dataloader=train_dataloader,\n",
    "             val_dataloader=val_dataloader,\n",
    "             test_dataloader=test_dataloader,\n",
    "             epochs=EPOCHS,\n",
    "             saved_model_name='model_dim-128x128_loss-crossentropy_optim-adam_lr-1e-3_batch-64.pth',\n",
    "             log_name='model_dim-128x128_loss-crossentropy_optim-adam_lr-1e-3_batch-64.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-hughes",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
